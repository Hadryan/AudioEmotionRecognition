from tensorflow.keras.layers import *from tensorflow.keras import Modelfrom data_loader import *from keras.utils import np_utilsfrom tensorflow.keras.callbacks import ReduceLROnPlateaufrom tensorflow.keras.optimizers import SGD, Adamdef normalization(data):    _range = np.max(data) - np.min(data)    return (data - np.min(data)) / _rangex_train, x_test, y_train, y_test = data_loader()x_train = normalization(x_train) -0.5x_test = normalization(x_test) -0.5y_train = np_utils.to_categorical(y_train)y_test = np_utils.to_categorical(y_test)shape_inputdata = [216, 1]input_data = Input(shape_inputdata)x = Conv1D(filters=512, kernel_size=1, padding='same', activation='relu')(input_data)x = BatchNormalization()(x)x = MaxPooling1D()(x)x = Conv1D(filters=512, kernel_size=1, activation='relu', padding='same')(x)x = BatchNormalization()(x)x = MaxPooling1D()(x)x = Conv1D(filters=512, kernel_size=3, activation='relu', padding='same')(x)x = MaxPooling1D()(x)x = Conv1D(filters=512, kernel_size=3, activation='relu', padding='same')(x)x = BatchNormalization()(x)x = MaxPooling1D()(x)x = Conv1D(filters=512, kernel_size=3, activation='relu', padding='same')(x)x = BatchNormalization()(x)x = Flatten()(x)x = Dense(1024, activation='relu')(x)x = Dropout(0.3)(x)x = Dense(9, activation='softmax')(x)AudioEmotionModel = Model(input_data, x)learning_rate_reduction = ReduceLROnPlateau(monitor='val_categorical_accuracy',                                            patience = 2,                                            verbose=1,                                            factor=0.5,                                            min_lr=0.00001)opt = Adam(learning_rate = 0.0001)AudioEmotionModel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['CategoricalAccuracy'])history = AudioEmotionModel.fit(x_train, y_train, batch_size=16,                                validation_data=(x_test, y_test),                                epochs=40, verbose=1,                                callbacks=[learning_rate_reduction])# score = AudioEmotionModel.evaluate(x_test, y_test, verbose=0)from matplotlib import pyplot as pltplt.figure()plt.plot(history.history['categorical_accuracy'])plt.plot(history.history['val_categorical_accuracy'])plt.title('Model accuracy')plt.ylabel('Accuracy')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig('Acc.png')plt.figure()# 绘制训练 & 验证的损失值plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model loss')plt.ylabel('Loss')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig('Loss.png')AudioEmotionModel.save('AudioEmotionModel-RAVDESS.h5')